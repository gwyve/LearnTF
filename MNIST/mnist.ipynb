{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#加载数据\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:   0.4075\n",
      "100:   0.8948\n",
      "200:   0.9031\n",
      "300:   0.9074\n",
      "400:   0.9037\n",
      "500:   0.9125\n",
      "600:   0.9163\n",
      "700:   0.9157\n",
      "800:   0.9158\n",
      "900:   0.9174\n",
      "#############################\n",
      "final:   0.9187\n"
     ]
    }
   ],
   "source": [
    "#一层全连接网络的\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=[None,784])\n",
    "y_true = tf.placeholder(tf.float32,shape=[None,10])\n",
    "\n",
    "#定义参数\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#定义预测值\n",
    "y = tf.matmul(x,W) + b\n",
    "\n",
    "#定义损失函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits =y))\n",
    "\n",
    "\n",
    "#求最佳\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "#开始训练\n",
    "for i in range(1000):\n",
    "    x_batch,y_batch = mnist.train.next_batch(100) #每次取100个\n",
    "    sess.run(train_step,feed_dict = {x:x_batch,y_true:y_batch})\n",
    "    if i % 100 == 0:\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_true,1))#产生一个长度为测试例子个数的bool数组，预测对了就是true\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) #tf.cast把bool类型转换成float32\n",
    "        print(str(i)+\":   \"+ str(sess.run(accuracy,{x:mnist.test.images,y_true:mnist.test.labels})))\n",
    "        \n",
    "print(\"#############################\")\n",
    "print(\"final:   \"+ str(sess.run(accuracy,{x:mnist.test.images,y_true:mnist.test.labels})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  0.1064\n",
      "100:  0.8286\n",
      "200:  0.9054\n",
      "300:  0.927\n",
      "400:  0.941\n",
      "500:  0.9437\n",
      "600:  0.95\n",
      "700:  0.9548\n",
      "800:  0.9563\n",
      "900:  0.9599\n",
      "1000:  0.963\n",
      "1100:  0.9643\n",
      "1200:  0.9684\n",
      "1300:  0.9678\n",
      "1400:  0.9694\n",
      "1500:  0.9701\n",
      "1600:  0.9738\n",
      "1700:  0.9736\n",
      "1800:  0.9743\n",
      "1900:  0.975\n",
      "2000:  0.9765\n",
      "2100:  0.9751\n",
      "2200:  0.9755\n",
      "2300:  0.9777\n",
      "2400:  0.9791\n",
      "2500:  0.9789\n",
      "2600:  0.9787\n",
      "2700:  0.9791\n",
      "2800:  0.9811\n",
      "2900:  0.9802\n",
      "3000:  0.9802\n",
      "3100:  0.9812\n",
      "3200:  0.9789\n",
      "3300:  0.9832\n",
      "3400:  0.9808\n",
      "3500:  0.9824\n",
      "3600:  0.9815\n",
      "3700:  0.9812\n",
      "3800:  0.9829\n",
      "3900:  0.9833\n",
      "4000:  0.9832\n",
      "4100:  0.9852\n",
      "4200:  0.9854\n",
      "4300:  0.984\n",
      "4400:  0.9845\n",
      "4500:  0.9853\n",
      "4600:  0.9825\n",
      "4700:  0.9858\n",
      "4800:  0.9855\n",
      "4900:  0.9849\n",
      "5000:  0.986\n",
      "5100:  0.9855\n",
      "5200:  0.9845\n",
      "5300:  0.9875\n",
      "5400:  0.9855\n",
      "5500:  0.9875\n",
      "5600:  0.9864\n",
      "5700:  0.9875\n",
      "5800:  0.9876\n",
      "5900:  0.9871\n",
      "6000:  0.9872\n",
      "6100:  0.9889\n",
      "6200:  0.9873\n",
      "6300:  0.9877\n",
      "6400:  0.9865\n",
      "6500:  0.9875\n",
      "6600:  0.9882\n",
      "6700:  0.9894\n",
      "6800:  0.9889\n",
      "6900:  0.9896\n",
      "7000:  0.9893\n",
      "7100:  0.9887\n",
      "7200:  0.9884\n",
      "7300:  0.9879\n",
      "7400:  0.9886\n",
      "7500:  0.9888\n",
      "7600:  0.9892\n",
      "7700:  0.9878\n",
      "7800:  0.9886\n",
      "7900:  0.9897\n",
      "8000:  0.9893\n",
      "8100:  0.9886\n",
      "8200:  0.9885\n",
      "8300:  0.9889\n",
      "8400:  0.9898\n",
      "8500:  0.9886\n",
      "8600:  0.9899\n",
      "8700:  0.9904\n",
      "8800:  0.9899\n",
      "8900:  0.9902\n",
      "9000:  0.991\n",
      "9100:  0.9901\n",
      "9200:  0.9916\n",
      "9300:  0.9903\n",
      "9400:  0.9909\n",
      "9500:  0.9901\n",
      "9600:  0.9913\n",
      "9700:  0.9909\n",
      "9800:  0.9899\n",
      "9900:  0.9901\n",
      "10000:  0.9904\n",
      "10100:  0.989\n",
      "10200:  0.9908\n",
      "10300:  0.9911\n",
      "10400:  0.9905\n",
      "10500:  0.9914\n",
      "10600:  0.9913\n",
      "10700:  0.9913\n",
      "10800:  0.9911\n",
      "10900:  0.9911\n",
      "11000:  0.9905\n",
      "11100:  0.9896\n",
      "11200:  0.991\n",
      "11300:  0.9916\n",
      "11400:  0.9907\n",
      "11500:  0.9915\n",
      "11600:  0.991\n",
      "11700:  0.9916\n",
      "11800:  0.9896\n",
      "11900:  0.989\n",
      "12000:  0.9908\n",
      "12100:  0.9911\n",
      "12200:  0.9911\n",
      "12300:  0.9904\n",
      "12400:  0.9919\n",
      "12500:  0.9928\n",
      "12600:  0.9918\n",
      "12700:  0.9919\n",
      "12800:  0.9903\n",
      "12900:  0.9921\n",
      "13000:  0.992\n",
      "13100:  0.9923\n",
      "13200:  0.9915\n",
      "13300:  0.9912\n",
      "13400:  0.9917\n",
      "13500:  0.9926\n",
      "13600:  0.988\n",
      "13700:  0.9912\n",
      "13800:  0.9913\n",
      "13900:  0.992\n",
      "14000:  0.992\n",
      "14100:  0.9924\n",
      "14200:  0.9922\n",
      "14300:  0.992\n",
      "14400:  0.9921\n",
      "14500:  0.9924\n",
      "14600:  0.9915\n",
      "14700:  0.9914\n",
      "14800:  0.9902\n",
      "14900:  0.991\n",
      "15000:  0.9909\n",
      "15100:  0.9918\n",
      "15200:  0.9923\n",
      "15300:  0.9923\n",
      "15400:  0.9922\n",
      "15500:  0.9916\n",
      "15600:  0.9921\n",
      "15700:  0.9925\n",
      "15800:  0.9924\n",
      "15900:  0.9923\n",
      "16000:  0.9922\n",
      "16100:  0.9929\n",
      "16200:  0.9918\n",
      "16300:  0.9913\n",
      "16400:  0.9927\n",
      "16500:  0.9922\n",
      "16600:  0.9924\n",
      "16700:  0.9913\n",
      "16800:  0.9917\n",
      "16900:  0.9915\n",
      "17000:  0.992\n",
      "17100:  0.9922\n",
      "17200:  0.9913\n",
      "17300:  0.9921\n",
      "17400:  0.9922\n",
      "17500:  0.9921\n",
      "17600:  0.9917\n",
      "17700:  0.9927\n",
      "17800:  0.9926\n",
      "17900:  0.9929\n",
      "18000:  0.992\n",
      "18100:  0.9922\n",
      "18200:  0.9933\n",
      "18300:  0.9918\n",
      "18400:  0.992\n",
      "18500:  0.9917\n",
      "18600:  0.9927\n",
      "18700:  0.9916\n",
      "18800:  0.9922\n",
      "18900:  0.9919\n",
      "19000:  0.9917\n",
      "19100:  0.9908\n",
      "19200:  0.9922\n",
      "19300:  0.9923\n",
      "19400:  0.9912\n",
      "19500:  0.9921\n",
      "19600:  0.9926\n",
      "19700:  0.9926\n",
      "19800:  0.9928\n",
      "19900:  0.9928\n",
      "#####################\n",
      "final:   0.9911\n"
     ]
    }
   ],
   "source": [
    "#使用卷积网络的方法\n",
    "\n",
    "#定义权重/卷积核\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1) #生成截断的自然分布,stddev是标准差\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#定义fc中的偏移量\n",
    "def bais_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)#这里偏移量用的是固定值\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#定义卷积层\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=\"SAME\") \n",
    "#这里x是输入的数据，W为卷积核，必须strides[0]=strides[3]=1,strides[1]是height的步长，strides[2]是width的步长。\n",
    "#padding 可以选两个参数，VALID是没有补，SAME是补0   http://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t\n",
    "#x、W为四维张量，x[batch, in_height, in_width, in_channels] ,W[filter_height, filter_width, in_channels, out_channels]\n",
    "\n",
    "\n",
    "#定义最大池化层\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")#这里的ksize与Strides类似\n",
    "\n",
    "#定义第一个卷基层的卷积核与偏移量\n",
    "W_conv1 = weight_variable([5,5,1,32]) #5x5x1的卷积核，输出32个通道\n",
    "b_conv1 = bais_variable([32])\n",
    "\n",
    "#变换原始数据\n",
    "x_image = tf.reshape(x,[-1,28,28,1]) #第一个维度为-1，代表第一个维度可以不限制大小，换句话说，就是图片的batch\n",
    "\n",
    "#第一层输出\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1) #通过relu激活函数\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "#写第二个卷基层\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bais_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#输入到fc中\n",
    "W_fc1 = weight_variable([7*7*64,1024])\n",
    "b_fc1 = bais_variable([1024])\n",
    "\n",
    "#第二个池化出来的数据降维\n",
    "h_pool2_plat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "\n",
    "#计算第一个fc层\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_plat,W_fc1) + b_fc1)\n",
    "\n",
    "#添加dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob) #keep_drop : The probability that each element is kept.\n",
    "\n",
    "\n",
    "#第二个fc层\n",
    "W_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bais_variable([10])\n",
    "\n",
    "\n",
    "#最后结果\n",
    "y_conv = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_conv))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_true,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20000):\n",
    "    x_batch,y_batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "        print(str(i)+\":  \"+ str(sess.run(accuracy,feed_dict={x:mnist.test.images,y_true:mnist.test.labels,keep_prob:1})))\n",
    "    sess.run(train_step,feed_dict={x:x_batch,y_true:y_batch,keep_prob:0.5})\n",
    "    #dropout在训练的时候随机失活，在预测的时候便不再失活。\n",
    "    \n",
    "    \n",
    "print(\"#####################\")\n",
    "print(\"final:   \" + str(sess.run(accuracy,feed_dict={x:mnist.test.images,y_true:mnist.test.labels,keep_prob:1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这个是我按照tensorflow（https://www.tensorflow.org/get_started/mnist/pros）这个教程上面写的。并且把每一行都看懂，并写下注释，可能对我自己有用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
